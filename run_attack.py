import pickle
from attacks import interweaveTexts, similarReplacement
from watermark import WatermarkedLLM
from datasegmenting import CreateDataset
from llms import LLM
import random

# Hyperparameters
NUM_TRIALS = 5
MODEL_NAME = 'openai-community/gpt2'
#MODEL_NAME = 'HuggingFaceH4/tiny-random-LlamaForCausalLM'
INIT_SENT = 'The following sentences are taken from the abstract of a scientific paper.'

human_evals = []
marked_evals = []
unmarked_evals = []

base_llm = LLM(MODEL_NAME)

def evaluateAttack(llm, attack):
    whole_str = attack
    id_list = llm.llm.str_to_idlist(whole_str)
    marked_logprob = llm.eval_log_prob(id_list)
    unmarked_logprob = llm.eval_log_prob(id_list, with_watermark=False)

    if unmarked_logprob < marked_logprob:
        attack_eval = "human"
    else:
        attack_eval = "watermarked"
    return attack_eval

for i in range(1):
    # Generate two LLMs, one watermarked and one not
    seed = random.randint(1, 9999)
    llm = WatermarkedLLM(base_llm, seed)

    seed2 = random.randint(1, 9999)
    llm2 = WatermarkedLLM(base_llm, seed)

    # Loop over datapoints
    dataset = CreateDataset('sometext.txt', INIT_SENT)
    dataset_len = len(dataset)
    count = 0
    ratios = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]
    attack1_results = {0: [], 0.1: [], 0.2: [], 0.3: [], 0.4: [], 0.5: [], 0.6: [], 0.7: [], 0.8: [], 0.9: [], 1: []}
    attack2_results = {0: [], 0.1: [], 0.2: [], 0.3: [], 0.4: [], 0.5: [], 0.6: [], 0.7: [], 0.8: [], 0.9: [], 1: []}
    attack3_results = {0: [], 0.1: [], 0.2: [], 0.3: [], 0.4: [], 0.5: [], 0.6: [], 0.7: [], 0.8: [], 0.9: [], 1: []}


    
    for j, datapoint in enumerate(dataset):
        # Extract datapoint
        prompt = datapoint['prompt']
        human_response = datapoint['rest']

        # Test whether our model classifies human response as human or AI-generated
        whole_str = prompt + ' ' + human_response
        id_list = llm.llm.str_to_idlist(whole_str)
        marked_logprob = llm.eval_log_prob(id_list)
        unmarked_logprob = llm.eval_log_prob(id_list, with_watermark=False)
        human_eval = (unmarked_logprob < marked_logprob) # Set to true if successful
        human_evals.append(human_eval)

        # Generate watermarked text
        id_list = llm.llm.str_to_idlist(prompt)
        marked_text = llm.autoregress_ids(id_list, gen_len=50, with_watermark=True)
        unmarked_text = llm.autoregress_ids(id_list, gen_len=50, with_watermark=False)

        # Remove text not generated by model
        marked_text = marked_text[len(id_list):]
        unmarked_text = unmarked_text[len(id_list):]

        # Test whether our model classifies watermarked text properly
        marked_logprob = llm.eval_log_prob(marked_text)
        unmarked_logprob = llm.eval_log_prob(marked_text, with_watermark=False)
        marked_eval = (unmarked_logprob < marked_logprob)
        #print("marked_eval: ", marked_eval)
        marked_evals.append(marked_eval)

        # Test whether our model classifies non-watermarked text properly
        marked_logprob = llm.eval_log_prob(unmarked_text)
        unmarked_logprob = llm.eval_log_prob(unmarked_text, with_watermark=False)
        unmarked_eval = (unmarked_logprob > marked_logprob)
        unmarked_evals.append(unmarked_eval)

        #attack 1
        dataset_choice2 = random.choice(dataset)
        prompt2 = dataset_choice2['prompt']

        id_list2 = llm2.llm.str_to_idlist(prompt2)
        marked_text2 = llm2.autoregress_ids(id_list2, gen_len=50, with_watermark=True)
        marked_text2 = marked_text2[len(id_list2):]
        
        for r in ratios:
            llm_output = base_llm.tokenlist_to_str(base_llm.idlist_to_tokenlist(marked_text)) 
            llm_output2 = base_llm.tokenlist_to_str(base_llm.idlist_to_tokenlist(marked_text2)) 
            attack = interweaveTexts(llm_output, llm_output2, r)
            
            attack_eval = evaluateAttack(llm, attack)
            attack1_results[r] += [attack_eval]

        
        for r in ratios:
            llm_output = base_llm.tokenlist_to_str(base_llm.idlist_to_tokenlist(marked_text)) 
            attack = interweaveTexts(llm_output, human_response, r)
            attack_eval = evaluateAttack(llm, attack)

            attack2_results[r] += attack_eval
            attack2_results[r] += [attack_eval]
        
        
        for r in ratios:
            llm_output = base_llm.tokenlist_to_str(base_llm.idlist_to_tokenlist(marked_text)) 
            attack = similarReplacement(llm_output, r)
            
            attack_eval = evaluateAttack(llm, attack)
            attack3_results[r] += [attack_eval]


    """
    with open("attack_results/attack1" + '.pickle', 'wb') as f:
                    pickle.dump(attack1_results, f, pickle.HIGHEST_PROTOCOL)
    with open("attack_results/attack2" + '.pickle', 'wb') as f:
                    pickle.dump(attack2_results, f, pickle.HIGHEST_PROTOCOL)
    with open("attack_results/attack3" + '.pickle', 'wb') as f:
                    pickle.dump(attack3_results, f, pickle.HIGHEST_PROTOCOL)
    """

